{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* Create an array of 3 parquet files to load\n",
    "    * Create a simple CSV, then read it in as a CSV\n",
    "* Create an outer loop of all the parquet files\n",
    "* For each parquet, create a delta table in databricks using Apache Spark\n",
    "    * https://www.projectpro.io/recipes/create-delta-table-with-existing-data-databricks\n",
    "```\n",
    "//reading source file and writing to destination path \n",
    "// Skip this since we are reading in a parquet \n",
    "spark.read.option(\"inferschema\",true).option(\"header\",true).csv(\"/FileStore/tables/sample_emp_data.txt\")\n",
    "\n",
    "// find a way to write a local delta table instead of the Databricks FileStore?\n",
    "write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/delta_train/\")\n",
    "//Below we are listing the data in destination path \n",
    "display(dbutils.fs.ls(\"/FileStore/tables/delta_train/\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import findspark\n",
    "from pyspark import DeltaTable\n",
    "from pyspark import spark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "# Small Parquet example\n",
    "filenames_list = [\"smallTable.csv\", \"largeTable.csv\"]\n",
    "parquetList = []\n",
    "\n",
    "for n, f in enumerate(filenames_list):\n",
    "    df = dd.read_csv(f, compression=\"gzip\", blocksize=None)\n",
    "    df = df.repartition(partition_size=\"100MB\")\n",
    "    df.write.format(\"parquet\").saveAsTable(\"parquetTable_\" + n)\n",
    "    parq = df.to_parquet(parquet_file, write_index=False)\n",
    "    parquetList.append(parq)\n",
    "\n",
    "\n",
    "for n, p in enumerate(parquetList):\n",
    "    df = pd.read_parquet(path, engine='auto', columns=None, storage_options=None, use_nullable_dtypes=False, **kwargs)\n",
    "    df.write.format(\"delta\").saveAsTable(\"deltaTable_\" + n)\n",
    "    DeltaTable.isDeltaTable(spark, \"deltaTable_\" + n) # True\n",
    "    spark.table(\"deltaTable_\" + n).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
