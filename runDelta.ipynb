{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* Create an array of 3 parquet files to load\n",
    "    * Create a simple CSV, then read it in as a CSV\n",
    "* Create an outer loop of all the parquet files\n",
    "* For each parquet, create a delta table in databricks using Apache Spark\n",
    "    * https://www.projectpro.io/recipes/create-delta-table-with-existing-data-databricks\n",
    "```\n",
    "//reading source file and writing to destination path \n",
    "// Skip this since we are reading in a parquet \n",
    "spark.read.option(\"inferschema\",true).option(\"header\",true).csv(\"/FileStore/tables/sample_emp_data.txt\")\n",
    "\n",
    "// find a way to write a local delta table instead of the Databricks FileStore?\n",
    "write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/delta_train/\")\n",
    "//Below we are listing the data in destination path \n",
    "display(dbutils.fs.ls(\"/FileStore/tables/delta_train/\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Parquet example\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
